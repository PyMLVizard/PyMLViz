{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "Implementation of gradient descent classes, using previous classes for sampling.\n",
    "I tried to keep the two classes Drawing and DrawingMethod and used a structure similar to the SamplingMethods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "import numpy as np\n",
    "from jupyter_cms.loader import load_notebook\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "import os\n",
    "path = os.getcwd()\n",
    "s = '/'\n",
    "pardir = s.join(path.split(s)[:-1])\n",
    "\n",
    "# Load source notebooks\n",
    "widget_targets = load_notebook(str(pardir + '/widgets/Widget_targets.ipynb'))\n",
    "widget_methods = load_notebook(str(pardir + '/widgets/Widget_methods.ipynb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class Target_GD(widget_targets.Target):\n",
    "    '''\n",
    "    Wrapper class around Target class. Required because Targets defined for \n",
    "    ascents, therefore need to inverse the gradient. \n",
    "    Additionally required to add stochastic noise, to simulate stochastic\n",
    "    gradient descent.\n",
    "    '''\n",
    "    def __init__(self, target, stochastic=0, stochastic_std=5):\n",
    "        self.target = target\n",
    "        self.stochastic = stochastic\n",
    "        self.stochastic_std = stochastic_std\n",
    "        \n",
    "    def grad(self, theta):\n",
    "        return -1 * self.target.grad(theta) + self.stochastic * np.random.normal(loc=0., scale=self.stochastic_std) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class GradientDescent(widget_methods.Drawing):\n",
    "    '''\n",
    "    Base class for all gradient descent algorithms\n",
    "    '''\n",
    "    def __init__(self, target=widget_targets.MultNorm(), learning_rate=0.1, \n",
    "                 num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.target_GD = Target_GD(target, stochastic, stochastic_std=5)\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        # Set the start point from the arguments or by default\n",
    "        self.default_start = np.array([1.5,-0.05])\n",
    "        if theta_start is not None:\n",
    "            self.theta_start = theta_start\n",
    "        else:\n",
    "            self.theta_start = np.copy(self.default_start)\n",
    "            \n",
    "        self.epochID = 0\n",
    "        # list to store updated parameters\n",
    "        self.thetas=[]\n",
    "        self.num_samples = 0\n",
    "        self.accepted = 0\n",
    "        # warning boolean, sent to widget\n",
    "        self.warning = False\n",
    "        \n",
    "    def reset_start(self, x=None):\n",
    "        '''\n",
    "        reset parameter values to default values or to that \n",
    "        specified by user\n",
    "        '''\n",
    "        if (x is None):\n",
    "            self.theta_start = self.default_start\n",
    "        else:\n",
    "            self.theta_start = x\n",
    "\n",
    "    def __str__(self):\n",
    "        pass\n",
    "        \n",
    "    def finalize_gradient_descent(self):\n",
    "        '''\n",
    "        remove those parameters outside of boundaries, \n",
    "        set count values, return numpy array of thetas\n",
    "        '''\n",
    "        \n",
    "        thetas = np.array(self.thetas)\n",
    "        \n",
    "        # number of computed steps\n",
    "        self.num_samples = thetas.shape[0]\n",
    "        \n",
    "        # only keep those steps that lie within the boundaries of the target distribution\n",
    "        thetas = thetas[(np.max(thetas, axis=1) < self.target_GD.target.get_size())\\\n",
    "                                                & (np.min(thetas, axis=1) > - self.target_GD.target.get_size())]\n",
    "        # number of steps to be plotted\n",
    "        self.accepted = thetas.shape[0]\n",
    "        \n",
    "        self.thetas = thetas\n",
    "          \n",
    "    def perform_gradient_descent(self):\n",
    "        '''iterate through samples'''\n",
    "        \n",
    "        theta = self.theta_start\n",
    "                \n",
    "        # perform updates\n",
    "        for self.epochID in np.arange(self.num_epochs):\n",
    "            \n",
    "            self.thetas.append(theta)\n",
    "            \n",
    "            # update parameters\n",
    "            theta = theta - self.comp_update(theta)\n",
    "            \n",
    "        # catch for inf thetas due to inf in gradient\n",
    "        if np.any(np.isinf(self.thetas)):\n",
    "            self.warning = True\n",
    "\n",
    "        self.finalize_gradient_descent()\n",
    "        \n",
    "        return self.thetas, self.warning\n",
    "    \n",
    "    def comp_update(self, theta):\n",
    "        '''\n",
    "        compute individual update for one step, \n",
    "        must be implemented by each subclass\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class VanillaGD(GradientDescent):\n",
    "    '''Vanilla gradient descent class'''\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"\\nVanilla gradient descent has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)       \n",
    "       \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)\n",
    "                \n",
    "        update = self.learning_rate * grad\n",
    "        \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MomentumGD(GradientDescent):\n",
    "    '''Gradient Descent with momentum.'''\n",
    "\n",
    "    def __init__(self, target=widget_targets.MultNorm(), gamma=0.9, learning_rate=0.1,\n",
    "                num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.velocity = np.zeros_like(theta_start, dtype='float')\n",
    "\n",
    "        super(MomentumGD, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"\\nMomentum gradient descent has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)       \n",
    "            \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)\n",
    "                \n",
    "        update = self.gamma * self.velocity + self.learning_rate * grad\n",
    "        \n",
    "        self.velocity = update\n",
    "        \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class NesterovGD(GradientDescent):\n",
    "    '''Nesterov accelerated gradient descent'''\n",
    "    \n",
    "    def __init__(self, target=widget_targets.MultNorm(), gamma=0.9, learning_rate=0.1,\n",
    "                num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.velocity = np.zeros_like(theta_start, dtype='float')\n",
    "        \n",
    "        super(NesterovGD, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\\nNesterov gradient descent has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)\n",
    "            \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        update = self.gamma * self.velocity + \\\n",
    "                    self.learning_rate * self.target_GD.grad(theta - self.gamma * self.velocity)\n",
    "        \n",
    "        self.velocity = update\n",
    "            \n",
    "        return update\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class ADAGRAD(GradientDescent):\n",
    "    '''Adagrad'''\n",
    "    \n",
    "    def __init__(self, target=widget_targets.MultNorm(), epsilon=1E-8, learning_rate=0.01,\n",
    "                num_epochs=20, theta_start=None, stochastic=0):\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.past_sq_grad = np.zeros_like(theta_start, dtype='float')\n",
    "        \n",
    "        super(ADAGRAD, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"\\ADAGRAD has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)\n",
    "            \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)\n",
    "        self.past_sq_grad += np.power(grad, 2)\n",
    "        \n",
    "        update = (self.learning_rate/(np.sqrt(self.past_sq_grad) + self.epsilon)) * grad\n",
    "            \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class RMSProp(GradientDescent):\n",
    "    '''Root Mean Square propagation'''\n",
    "\n",
    "    def __init__(self, target=widget_targets.MultNorm(), gamma=0.9, epsilon=1E-8, learning_rate=0.1,\n",
    "                num_epochs=20, theta_start=None, stochastic=0):  \n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.avg_sq_grad = np.zeros_like(theta_start, dtype='float')\n",
    "        \n",
    "        super(RMSProp, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"\\nRMSProp has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)       \n",
    "    \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)                    \n",
    "        self.avg_sq_grad = self.gamma * self.avg_sq_grad + (1-self.gamma) * np.power(grad, 2)\n",
    "        \n",
    "        update = (self.learning_rate/(np.sqrt(self.avg_sq_grad) + self.epsilon)) * grad\n",
    "            \n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class ADAM(GradientDescent):\n",
    "    '''Adaptive Moment Estimation'''\n",
    "\n",
    "    def __init__(self, target=widget_targets.MultNorm(), beta1=0.9, beta2=0.999, epsilon=1E-8,\n",
    "                 learning_rate=0.1, num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # initialize moment estimates\n",
    "        self.est_mom_1 = np.zeros_like(theta_start, dtype='float')\n",
    "        self.est_mom_2 = np.zeros_like(theta_start, dtype='float')\n",
    "\n",
    "\n",
    "        super(ADAM, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"\\nADAM has performed \"+\\\n",
    "            \"%d steps with %d lieing within the depicted boundaries.\" % (self.num_samples, self.accepted)       \n",
    "        \n",
    "    def comp_update(self, theta):\n",
    "        \n",
    "        grad = self.target_GD.grad(theta)\n",
    "                    \n",
    "        self.est_mom_1 = self.beta1 * self.est_mom_1 + (1-self.beta1) * grad\n",
    "        self.est_mom_2 = self.beta2 * self.est_mom_2 + (1-self.beta2) * np.power(grad, 2)\n",
    "            \n",
    "        # bias corrected decaying averages\n",
    "        unbiased_est_mom_1 = self.est_mom_1/(1 - np.power(self.beta1, self.epochID+1))\n",
    "        unbiased_est_mom_2 = self.est_mom_2/(1 - np.power(self.beta2, self.epochID+1))\n",
    "\n",
    "        update = self.learning_rate/(np.sqrt(unbiased_est_mom_2) + self.epsilon) \\\n",
    "                                * unbiased_est_mom_1\n",
    "            \n",
    "        return update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradientDescentMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class GradientDescentMethod(widget_methods.DrawingMethod):\n",
    "    '''\n",
    "    A parent class for different gradient methods, used \n",
    "    specifically by animation widget. This class is required as \n",
    "    a wrapper around the gradient descent classes to make \n",
    "    resetting in the animation widget easier.\n",
    "    '''  \n",
    "          \n",
    "    def __init__(self, target=widget_targets.MultNorm(), learning_rate=0.1, \n",
    "                 num_epochs=20, theta_start=None, stochastic=0):\n",
    "        self.target = target\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        if theta_start is None:\n",
    "            self.theta_start = np.array([1.5,-0.05])\n",
    "        else:\n",
    "            self.theta_start = theta_start\n",
    "        \n",
    "        self.stochastic = stochastic\n",
    "        \n",
    "        self.reset_gradient_descent()    \n",
    "    \n",
    "    def reset_gradient_descent(self):\n",
    "        '''\n",
    "        Resets the gradient descent object with given parameters. \n",
    "        Must be implemented by every child class.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    def draw(self, start_point=None):  \n",
    "        '''\n",
    "        Performs the actual iteration of gradient descent with \n",
    "        the object that was specified in reset_gradient_descent().\n",
    "        '''\n",
    "        self.gradient_descent.reset_start(x=start_point)\n",
    "        self.data, self.warning = self.gradient_descent.perform_gradient_descent()\n",
    "        return {'accepted points' : self.data, 'warning' : self.warning, \n",
    "                'epochs' : self.num_epochs, 'learning_rate': self.learning_rate}\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.gradient_descent.__str__()\n",
    "    \n",
    "    def set_param(self, param_dict):\n",
    "        '''\n",
    "        Allows to set additional gradient descent parameters, given from extra_widget\n",
    "        '''\n",
    "        for i in param_dict:\n",
    "            if i =='learning_rate':\n",
    "                self.learning_rate = param_dict['learning_rate']\n",
    "            elif i=='epochs':\n",
    "                self.num_epochs = param_dict['epochs']\n",
    "            elif i=='target':\n",
    "                self.target = param_dict['target']\n",
    "            elif i=='x':                \n",
    "                self.theta_start[0] = param_dict['x']\n",
    "            elif i=='y':                \n",
    "                self.theta_start[1] = param_dict['y']\n",
    "            elif i=='gamma':\n",
    "                self.gamma = param_dict['gamma']\n",
    "            elif i=='beta1':\n",
    "                self.beta1 = param_dict['beta1']\n",
    "            elif i=='beta2':\n",
    "                self.beta2 = param_dict['beta2']\n",
    "            elif i=='stochastic':\n",
    "                self.stochastic = param_dict['stochastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class VanillaGDM(GradientDescentMethod):\n",
    "    '''Vanilla, aka batch gradient descent method class'''\n",
    "           \n",
    "    def reset_gradient_descent(self): \n",
    "        self.gradient_descent = VanillaGD(self.target, self.learning_rate, self.num_epochs, self.theta_start,\n",
    "                                          self.stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class MomentumGDM(GradientDescentMethod):\n",
    "    '''GD with Momentum method.'''\n",
    "\n",
    "    def __init__(self, target=widget_targets.MultNorm(), gamma=0.9,\n",
    "                 learning_rate=0.1, num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        super(MomentumGDM, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "           \n",
    "    def reset_gradient_descent(self): \n",
    "        self.gradient_descent = MomentumGD(self.target, self.gamma, self.learning_rate,\n",
    "                                           self.num_epochs, self.theta_start, self.stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class NesterovGDM(GradientDescentMethod):\n",
    "    '''GD with Nesterov method.'''\n",
    "\n",
    "    def __init__(self, target=widget_targets.MultNorm(), gamma=0.9,\n",
    "                        learning_rate=0.1, num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        super(NesterovGDM, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "           \n",
    "    def reset_gradient_descent(self): \n",
    "        self.gradient_descent = NesterovGD(self.target, self.gamma, self.learning_rate,\n",
    "                                           self.num_epochs, self.theta_start, self.stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class ADAGRADM(GradientDescentMethod):\n",
    "    '''Adagrad method'''\n",
    "    \n",
    "    def __init__(self, target=widget_targets.MultNorm(), epsilon=1E-8, \n",
    "                         learning_rate=0.01, num_epochs=20, theta_start=None, stochastic=0):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        super(ADAGRADM, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "    \n",
    "    def reset_gradient_descent(self): \n",
    "        self.gradient_descent = ADAGRAD(self.target, self.epsilon, self.learning_rate, self.num_epochs, \n",
    "                                        self.theta_start, self.stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class RMSPropM(GradientDescentMethod):\n",
    "    '''Root Mean Square Propagation method.'''\n",
    "\n",
    "    def __init__(self, target=widget_targets.MultNorm(), gamma=0.9, epsilon=1E-8, \n",
    "                         learning_rate=0.1, num_epochs=20, theta_start=None, stochastic=0):\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        super(RMSPropM, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "            \n",
    "    def reset_gradient_descent(self): \n",
    "        self.gradient_descent = RMSProp(self.target, self.gamma, self.epsilon, self.learning_rate, \n",
    "                                        self.num_epochs, self.theta_start, self.stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<api>\n",
    "class ADAMM(GradientDescentMethod):\n",
    "    '''Adaptive Moment Estimation Method'''\n",
    "\n",
    "    def __init__(self, target=widget_targets.MultNorm(), beta1=0.9, beta2=0.999, epsilon=1E-8, \n",
    "                         learning_rate=0.1, num_epochs=20, theta_start=None, stochastic=0):\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        super(ADAMM, self).__init__(target, learning_rate, num_epochs, theta_start, stochastic)\n",
    "           \n",
    "    def reset_gradient_descent(self): \n",
    "        self.gradient_descent = ADAM(self.target, self.beta1, self.beta2, self.epsilon, \n",
    "                                     self.learning_rate, self.num_epochs, self.theta_start, \n",
    "                                     self.stochastic)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "75ca3631171d46738fd25e5ed8f5c01f",
   "lastKernelId": "f83a4590-e997-4bf1-94bd-f02bec29b7af"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
